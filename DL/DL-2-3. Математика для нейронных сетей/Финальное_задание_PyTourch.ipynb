{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peKwOLXOwes_"
      },
      "source": [
        "Давайте с вами решим многоклассовую задачу, но теперь с использованием более верхнеуровневого фреймворка PyTorch.\n",
        "\n",
        "Для этого мы возьмём датасет MNIST. Он очень похож на Digits из предыдущего задания и тоже содержит черно-белые рукописные цифры от 0 до 9, только размер уже 28х28 пикселей.\n",
        "\n",
        "Вообще, MNIST — знаковый датасет для глубокого обучения, именно для него Yahn LeCun разработал первую свёрточную нейросеть LeNet и применил для её обучения механизм обратного распространения ошибки, что дало сильный толчок в развитии нейронных сетей и глубокого обучения в целом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8kPwjr1yTQx"
      },
      "source": [
        "В этом задании вам придется много опираться на ноутбук ipynb, в котором мы знакомились с основными элементами PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ2UERTwkkUf"
      },
      "source": [
        "# Финальное задание PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb7nZCNfiHvX"
      },
      "source": [
        "Начнем с основных импортов и определения констант - доступного девайса для вычислений и параметров, которые зависят от выборки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XVmBHU5edzqN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# выбор девайса\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# параметры, которые зависят от нашего датасета\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "print(f'Using {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZDEIQ1JiVXm"
      },
      "source": [
        "Предлагаем вам попробовать выбрать самим гиперпараметры обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "id0sYfupd20V"
      },
      "outputs": [],
      "source": [
        "# выберите сами данные параметры\n",
        "num_epochs = 30\n",
        "batch_size = 32\n",
        "learning_rate = 1e-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnootprMixZm"
      },
      "source": [
        "Загружаем датасет MNIST - он есть в стандартном наборе датасетов pytorch, выбираем transform=ToTensor(), это важно!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cKVuwQcqd5Ql"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 18516979.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 62829716.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 6191686.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 15928535.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# MNIST dataset \n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# transform.ToTensor() записывает данные в torch.tensor и нормализовывает данные,\n",
        "# разделяя значения каждого пикселя на 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipgirQxSjKWG"
      },
      "source": [
        "Настраиваем dataloader - объект, который датасет "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "06yWRBfWjIfV"
      },
      "outputs": [],
      "source": [
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hD-iGbicd9FM"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.sequential = nn.Sequential(\n",
        "            nn.Linear(input_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "        # сконструируйте свою сеть, опишите слои, которые будут использоваться\n",
        "        # достаточно будет двух линейных слоев с ReLU после первого,\n",
        "        # но мы оставляем этот выбор вам, можно составить любую сеть\n",
        "        # SoftMax указывать не нужно \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.sequential(self.flatten(x))\n",
        "        # опишите функцию forward, то есть проход по вашей сети\n",
        "        # результат применения x к первому слою отдайте на вход функции активации\n",
        "        # выход функции активации отправьте на вход следущему слою и так далее\n",
        "        return out\n",
        "\n",
        "model = NeuralNet(input_size, num_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lBBvPF5ieAV0"
      },
      "outputs": [],
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "# в качестве функции потерь используем Cross Entropy\n",
        "# в PyTorch кросс энтропия реализована с помощью LogSoftmax и \n",
        "# Negative Log Likelyhood loss, что на самом деле даёт такой же результат\n",
        "# Так как мы не указали внутри сети SoftMax и он учтен только в Loss функции,\n",
        "# при тестировании сети нам нужно учесть этот факт и руками интерпретировать выход сети\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)# выберите оптимизатор (вариантов много - SGD, AdaGrad, Adam, RMSProp и т.д.)\n",
        "# параметры для оптимизатора, передайте в скобках в предыдущей строке(model.parameters(), lr=learning_rate)  \n",
        "# почитать про оптимизаторы можно тут https://pytorch.org/docs/stable/optim.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 1, 28, 28])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = next(iter(train_loader))\n",
        "test[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jaAOxTqKeDXM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Step [100/1875], Loss: 0.1519\n",
            "Epoch [1/30], Step [200/1875], Loss: 0.5406\n",
            "Epoch [1/30], Step [300/1875], Loss: 0.1115\n",
            "Epoch [1/30], Step [400/1875], Loss: 0.5553\n",
            "Epoch [1/30], Step [500/1875], Loss: 0.3290\n",
            "Epoch [1/30], Step [600/1875], Loss: 0.3371\n",
            "Epoch [1/30], Step [700/1875], Loss: 0.1706\n",
            "Epoch [1/30], Step [800/1875], Loss: 0.3104\n",
            "Epoch [1/30], Step [900/1875], Loss: 0.2279\n",
            "Epoch [1/30], Step [1000/1875], Loss: 0.1755\n",
            "Epoch [1/30], Step [1100/1875], Loss: 0.1396\n",
            "Epoch [1/30], Step [1200/1875], Loss: 0.1996\n",
            "Epoch [1/30], Step [1300/1875], Loss: 0.1368\n",
            "Epoch [1/30], Step [1400/1875], Loss: 0.1426\n",
            "Epoch [1/30], Step [1500/1875], Loss: 0.1220\n",
            "Epoch [1/30], Step [1600/1875], Loss: 0.3083\n",
            "Epoch [1/30], Step [1700/1875], Loss: 0.1461\n",
            "Epoch [1/30], Step [1800/1875], Loss: 0.1475\n",
            "Epoch [2/30], Step [100/1875], Loss: 0.0670\n",
            "Epoch [2/30], Step [200/1875], Loss: 0.2417\n",
            "Epoch [2/30], Step [300/1875], Loss: 0.1521\n",
            "Epoch [2/30], Step [400/1875], Loss: 0.0802\n",
            "Epoch [2/30], Step [500/1875], Loss: 0.1035\n",
            "Epoch [2/30], Step [600/1875], Loss: 0.0038\n",
            "Epoch [2/30], Step [700/1875], Loss: 0.0213\n",
            "Epoch [2/30], Step [800/1875], Loss: 0.1700\n",
            "Epoch [2/30], Step [900/1875], Loss: 0.1428\n",
            "Epoch [2/30], Step [1000/1875], Loss: 0.0264\n",
            "Epoch [2/30], Step [1100/1875], Loss: 0.3787\n",
            "Epoch [2/30], Step [1200/1875], Loss: 0.0205\n",
            "Epoch [2/30], Step [1300/1875], Loss: 0.1048\n",
            "Epoch [2/30], Step [1400/1875], Loss: 0.1515\n",
            "Epoch [2/30], Step [1500/1875], Loss: 0.0475\n",
            "Epoch [2/30], Step [1600/1875], Loss: 0.0569\n",
            "Epoch [2/30], Step [1700/1875], Loss: 0.0592\n",
            "Epoch [2/30], Step [1800/1875], Loss: 0.1397\n",
            "Epoch [3/30], Step [100/1875], Loss: 0.1373\n",
            "Epoch [3/30], Step [200/1875], Loss: 0.0643\n",
            "Epoch [3/30], Step [300/1875], Loss: 0.0715\n",
            "Epoch [3/30], Step [400/1875], Loss: 0.0904\n",
            "Epoch [3/30], Step [500/1875], Loss: 0.0878\n",
            "Epoch [3/30], Step [600/1875], Loss: 0.0319\n",
            "Epoch [3/30], Step [700/1875], Loss: 0.1414\n",
            "Epoch [3/30], Step [800/1875], Loss: 0.0725\n",
            "Epoch [3/30], Step [900/1875], Loss: 0.1163\n",
            "Epoch [3/30], Step [1000/1875], Loss: 0.0964\n",
            "Epoch [3/30], Step [1100/1875], Loss: 0.2312\n",
            "Epoch [3/30], Step [1200/1875], Loss: 0.0638\n",
            "Epoch [3/30], Step [1300/1875], Loss: 0.2785\n",
            "Epoch [3/30], Step [1400/1875], Loss: 0.0033\n",
            "Epoch [3/30], Step [1500/1875], Loss: 0.0953\n",
            "Epoch [3/30], Step [1600/1875], Loss: 0.0149\n",
            "Epoch [3/30], Step [1700/1875], Loss: 0.4266\n",
            "Epoch [3/30], Step [1800/1875], Loss: 0.1035\n",
            "Epoch [4/30], Step [100/1875], Loss: 0.0177\n",
            "Epoch [4/30], Step [200/1875], Loss: 0.0142\n",
            "Epoch [4/30], Step [300/1875], Loss: 0.0615\n",
            "Epoch [4/30], Step [400/1875], Loss: 0.5608\n",
            "Epoch [4/30], Step [500/1875], Loss: 0.0342\n",
            "Epoch [4/30], Step [600/1875], Loss: 0.2817\n",
            "Epoch [4/30], Step [700/1875], Loss: 0.2785\n",
            "Epoch [4/30], Step [800/1875], Loss: 0.2340\n",
            "Epoch [4/30], Step [900/1875], Loss: 0.1150\n",
            "Epoch [4/30], Step [1000/1875], Loss: 0.0022\n",
            "Epoch [4/30], Step [1100/1875], Loss: 0.0023\n",
            "Epoch [4/30], Step [1200/1875], Loss: 0.2687\n",
            "Epoch [4/30], Step [1300/1875], Loss: 0.0060\n",
            "Epoch [4/30], Step [1400/1875], Loss: 0.0021\n",
            "Epoch [4/30], Step [1500/1875], Loss: 0.0130\n",
            "Epoch [4/30], Step [1600/1875], Loss: 0.0504\n",
            "Epoch [4/30], Step [1700/1875], Loss: 0.2902\n",
            "Epoch [4/30], Step [1800/1875], Loss: 0.0645\n",
            "Epoch [5/30], Step [100/1875], Loss: 0.0119\n",
            "Epoch [5/30], Step [200/1875], Loss: 0.0225\n",
            "Epoch [5/30], Step [300/1875], Loss: 0.0003\n",
            "Epoch [5/30], Step [400/1875], Loss: 0.3364\n",
            "Epoch [5/30], Step [500/1875], Loss: 0.0556\n",
            "Epoch [5/30], Step [600/1875], Loss: 0.0107\n",
            "Epoch [5/30], Step [700/1875], Loss: 0.4120\n",
            "Epoch [5/30], Step [800/1875], Loss: 0.0446\n",
            "Epoch [5/30], Step [900/1875], Loss: 0.2894\n",
            "Epoch [5/30], Step [1000/1875], Loss: 0.0157\n",
            "Epoch [5/30], Step [1100/1875], Loss: 0.0391\n",
            "Epoch [5/30], Step [1200/1875], Loss: 0.1582\n",
            "Epoch [5/30], Step [1300/1875], Loss: 0.0338\n",
            "Epoch [5/30], Step [1400/1875], Loss: 0.0298\n",
            "Epoch [5/30], Step [1500/1875], Loss: 0.0812\n",
            "Epoch [5/30], Step [1600/1875], Loss: 0.1439\n",
            "Epoch [5/30], Step [1700/1875], Loss: 0.0455\n",
            "Epoch [5/30], Step [1800/1875], Loss: 0.0136\n",
            "Epoch [6/30], Step [100/1875], Loss: 0.1982\n",
            "Epoch [6/30], Step [200/1875], Loss: 0.1353\n",
            "Epoch [6/30], Step [300/1875], Loss: 0.1394\n",
            "Epoch [6/30], Step [400/1875], Loss: 0.2757\n",
            "Epoch [6/30], Step [500/1875], Loss: 0.1431\n",
            "Epoch [6/30], Step [600/1875], Loss: 0.2508\n",
            "Epoch [6/30], Step [700/1875], Loss: 0.0830\n",
            "Epoch [6/30], Step [800/1875], Loss: 0.0095\n",
            "Epoch [6/30], Step [900/1875], Loss: 0.0949\n",
            "Epoch [6/30], Step [1000/1875], Loss: 0.0039\n",
            "Epoch [6/30], Step [1100/1875], Loss: 0.0838\n",
            "Epoch [6/30], Step [1200/1875], Loss: 0.0299\n",
            "Epoch [6/30], Step [1300/1875], Loss: 0.1746\n",
            "Epoch [6/30], Step [1400/1875], Loss: 0.0006\n",
            "Epoch [6/30], Step [1500/1875], Loss: 0.0287\n",
            "Epoch [6/30], Step [1600/1875], Loss: 0.1832\n",
            "Epoch [6/30], Step [1700/1875], Loss: 0.0073\n",
            "Epoch [6/30], Step [1800/1875], Loss: 0.0012\n",
            "Epoch [7/30], Step [100/1875], Loss: 0.0850\n",
            "Epoch [7/30], Step [200/1875], Loss: 0.0810\n",
            "Epoch [7/30], Step [300/1875], Loss: 0.0022\n",
            "Epoch [7/30], Step [400/1875], Loss: 0.0005\n",
            "Epoch [7/30], Step [500/1875], Loss: 0.0545\n",
            "Epoch [7/30], Step [600/1875], Loss: 0.0005\n",
            "Epoch [7/30], Step [700/1875], Loss: 0.0167\n",
            "Epoch [7/30], Step [800/1875], Loss: 0.2883\n",
            "Epoch [7/30], Step [900/1875], Loss: 0.0025\n",
            "Epoch [7/30], Step [1000/1875], Loss: 0.0052\n",
            "Epoch [7/30], Step [1100/1875], Loss: 0.0037\n",
            "Epoch [7/30], Step [1200/1875], Loss: 0.0004\n",
            "Epoch [7/30], Step [1300/1875], Loss: 0.0761\n",
            "Epoch [7/30], Step [1400/1875], Loss: 0.0453\n",
            "Epoch [7/30], Step [1500/1875], Loss: 0.0006\n",
            "Epoch [7/30], Step [1600/1875], Loss: 0.0987\n",
            "Epoch [7/30], Step [1700/1875], Loss: 0.0036\n",
            "Epoch [7/30], Step [1800/1875], Loss: 0.2305\n",
            "Epoch [8/30], Step [100/1875], Loss: 0.1115\n",
            "Epoch [8/30], Step [200/1875], Loss: 0.0156\n",
            "Epoch [8/30], Step [300/1875], Loss: 0.0046\n",
            "Epoch [8/30], Step [400/1875], Loss: 0.0257\n",
            "Epoch [8/30], Step [500/1875], Loss: 0.0435\n",
            "Epoch [8/30], Step [600/1875], Loss: 0.0207\n",
            "Epoch [8/30], Step [700/1875], Loss: 0.0104\n",
            "Epoch [8/30], Step [800/1875], Loss: 0.1051\n",
            "Epoch [8/30], Step [900/1875], Loss: 0.1508\n",
            "Epoch [8/30], Step [1000/1875], Loss: 0.2287\n",
            "Epoch [8/30], Step [1100/1875], Loss: 0.1641\n",
            "Epoch [8/30], Step [1200/1875], Loss: 0.5197\n",
            "Epoch [8/30], Step [1300/1875], Loss: 0.3494\n",
            "Epoch [8/30], Step [1400/1875], Loss: 0.0010\n",
            "Epoch [8/30], Step [1500/1875], Loss: 0.1263\n",
            "Epoch [8/30], Step [1600/1875], Loss: 0.2721\n",
            "Epoch [8/30], Step [1700/1875], Loss: 0.3029\n",
            "Epoch [8/30], Step [1800/1875], Loss: 0.1909\n",
            "Epoch [9/30], Step [100/1875], Loss: 0.0157\n",
            "Epoch [9/30], Step [200/1875], Loss: 0.1931\n",
            "Epoch [9/30], Step [300/1875], Loss: 0.0123\n",
            "Epoch [9/30], Step [400/1875], Loss: 0.0322\n",
            "Epoch [9/30], Step [500/1875], Loss: 0.0248\n",
            "Epoch [9/30], Step [600/1875], Loss: 0.0293\n",
            "Epoch [9/30], Step [700/1875], Loss: 0.1767\n",
            "Epoch [9/30], Step [800/1875], Loss: 0.0648\n",
            "Epoch [9/30], Step [900/1875], Loss: 0.0018\n",
            "Epoch [9/30], Step [1000/1875], Loss: 0.0811\n",
            "Epoch [9/30], Step [1100/1875], Loss: 1.8772\n",
            "Epoch [9/30], Step [1200/1875], Loss: 0.0273\n",
            "Epoch [9/30], Step [1300/1875], Loss: 0.0134\n",
            "Epoch [9/30], Step [1400/1875], Loss: 0.0190\n",
            "Epoch [9/30], Step [1500/1875], Loss: 0.2843\n",
            "Epoch [9/30], Step [1600/1875], Loss: 0.0155\n",
            "Epoch [9/30], Step [1700/1875], Loss: 0.3068\n",
            "Epoch [9/30], Step [1800/1875], Loss: 0.1009\n",
            "Epoch [10/30], Step [100/1875], Loss: 0.1984\n",
            "Epoch [10/30], Step [200/1875], Loss: 0.0049\n",
            "Epoch [10/30], Step [300/1875], Loss: 0.0004\n",
            "Epoch [10/30], Step [400/1875], Loss: 0.0149\n",
            "Epoch [10/30], Step [500/1875], Loss: 0.0026\n",
            "Epoch [10/30], Step [600/1875], Loss: 0.0041\n",
            "Epoch [10/30], Step [700/1875], Loss: 0.1995\n",
            "Epoch [10/30], Step [800/1875], Loss: 0.2062\n",
            "Epoch [10/30], Step [900/1875], Loss: 0.0536\n",
            "Epoch [10/30], Step [1000/1875], Loss: 0.2854\n",
            "Epoch [10/30], Step [1100/1875], Loss: 0.1308\n",
            "Epoch [10/30], Step [1200/1875], Loss: 0.0003\n",
            "Epoch [10/30], Step [1300/1875], Loss: 0.0009\n",
            "Epoch [10/30], Step [1400/1875], Loss: 0.3470\n",
            "Epoch [10/30], Step [1500/1875], Loss: 0.2998\n",
            "Epoch [10/30], Step [1600/1875], Loss: 0.0505\n",
            "Epoch [10/30], Step [1700/1875], Loss: 0.0053\n",
            "Epoch [10/30], Step [1800/1875], Loss: 0.1113\n",
            "Epoch [11/30], Step [100/1875], Loss: 0.1090\n",
            "Epoch [11/30], Step [200/1875], Loss: 0.0555\n",
            "Epoch [11/30], Step [300/1875], Loss: 0.0406\n",
            "Epoch [11/30], Step [400/1875], Loss: 0.0372\n",
            "Epoch [11/30], Step [500/1875], Loss: 0.0082\n",
            "Epoch [11/30], Step [600/1875], Loss: 0.0343\n",
            "Epoch [11/30], Step [700/1875], Loss: 0.0012\n",
            "Epoch [11/30], Step [800/1875], Loss: 0.0389\n",
            "Epoch [11/30], Step [900/1875], Loss: 0.0001\n",
            "Epoch [11/30], Step [1000/1875], Loss: 0.0371\n",
            "Epoch [11/30], Step [1100/1875], Loss: 0.0003\n",
            "Epoch [11/30], Step [1200/1875], Loss: 0.0440\n",
            "Epoch [11/30], Step [1300/1875], Loss: 0.1401\n",
            "Epoch [11/30], Step [1400/1875], Loss: 0.0217\n",
            "Epoch [11/30], Step [1500/1875], Loss: 0.0671\n",
            "Epoch [11/30], Step [1600/1875], Loss: 0.1320\n",
            "Epoch [11/30], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [11/30], Step [1800/1875], Loss: 0.0003\n",
            "Epoch [12/30], Step [100/1875], Loss: 0.0544\n",
            "Epoch [12/30], Step [200/1875], Loss: 0.3316\n",
            "Epoch [12/30], Step [300/1875], Loss: 0.4972\n",
            "Epoch [12/30], Step [400/1875], Loss: 0.0003\n",
            "Epoch [12/30], Step [500/1875], Loss: 0.1052\n",
            "Epoch [12/30], Step [600/1875], Loss: 0.0608\n",
            "Epoch [12/30], Step [700/1875], Loss: 0.0032\n",
            "Epoch [12/30], Step [800/1875], Loss: 0.0001\n",
            "Epoch [12/30], Step [900/1875], Loss: 0.0281\n",
            "Epoch [12/30], Step [1000/1875], Loss: 0.0002\n",
            "Epoch [12/30], Step [1100/1875], Loss: 0.0017\n",
            "Epoch [12/30], Step [1200/1875], Loss: 0.1662\n",
            "Epoch [12/30], Step [1300/1875], Loss: 0.2176\n",
            "Epoch [12/30], Step [1400/1875], Loss: 0.2642\n",
            "Epoch [12/30], Step [1500/1875], Loss: 0.0061\n",
            "Epoch [12/30], Step [1600/1875], Loss: 0.0209\n",
            "Epoch [12/30], Step [1700/1875], Loss: 0.0141\n",
            "Epoch [12/30], Step [1800/1875], Loss: 0.0002\n",
            "Epoch [13/30], Step [100/1875], Loss: 0.0001\n",
            "Epoch [13/30], Step [200/1875], Loss: 0.0008\n",
            "Epoch [13/30], Step [300/1875], Loss: 0.0047\n",
            "Epoch [13/30], Step [400/1875], Loss: 0.0005\n",
            "Epoch [13/30], Step [500/1875], Loss: 0.1169\n",
            "Epoch [13/30], Step [600/1875], Loss: 0.4753\n",
            "Epoch [13/30], Step [700/1875], Loss: 0.0779\n",
            "Epoch [13/30], Step [800/1875], Loss: 0.1970\n",
            "Epoch [13/30], Step [900/1875], Loss: 0.0049\n",
            "Epoch [13/30], Step [1000/1875], Loss: 0.0001\n",
            "Epoch [13/30], Step [1100/1875], Loss: 0.0372\n",
            "Epoch [13/30], Step [1200/1875], Loss: 0.0099\n",
            "Epoch [13/30], Step [1300/1875], Loss: 0.0115\n",
            "Epoch [13/30], Step [1400/1875], Loss: 0.0308\n",
            "Epoch [13/30], Step [1500/1875], Loss: 0.0086\n",
            "Epoch [13/30], Step [1600/1875], Loss: 0.0525\n",
            "Epoch [13/30], Step [1700/1875], Loss: 0.0392\n",
            "Epoch [13/30], Step [1800/1875], Loss: 0.7839\n",
            "Epoch [14/30], Step [100/1875], Loss: 0.0374\n",
            "Epoch [14/30], Step [200/1875], Loss: 0.1863\n",
            "Epoch [14/30], Step [300/1875], Loss: 0.0026\n",
            "Epoch [14/30], Step [400/1875], Loss: 0.0096\n",
            "Epoch [14/30], Step [500/1875], Loss: 0.0017\n",
            "Epoch [14/30], Step [600/1875], Loss: 0.0000\n",
            "Epoch [14/30], Step [700/1875], Loss: 0.2629\n",
            "Epoch [14/30], Step [800/1875], Loss: 0.0000\n",
            "Epoch [14/30], Step [900/1875], Loss: 0.0077\n",
            "Epoch [14/30], Step [1000/1875], Loss: 0.0012\n",
            "Epoch [14/30], Step [1100/1875], Loss: 0.0018\n",
            "Epoch [14/30], Step [1200/1875], Loss: 0.2301\n",
            "Epoch [14/30], Step [1300/1875], Loss: 0.0019\n",
            "Epoch [14/30], Step [1400/1875], Loss: 1.2957\n",
            "Epoch [14/30], Step [1500/1875], Loss: 0.0199\n",
            "Epoch [14/30], Step [1600/1875], Loss: 0.0005\n",
            "Epoch [14/30], Step [1700/1875], Loss: 0.0381\n",
            "Epoch [14/30], Step [1800/1875], Loss: 0.0001\n",
            "Epoch [15/30], Step [100/1875], Loss: 0.0000\n",
            "Epoch [15/30], Step [200/1875], Loss: 0.0002\n",
            "Epoch [15/30], Step [300/1875], Loss: 0.1535\n",
            "Epoch [15/30], Step [400/1875], Loss: 0.2565\n",
            "Epoch [15/30], Step [500/1875], Loss: 0.4669\n",
            "Epoch [15/30], Step [600/1875], Loss: 0.0385\n",
            "Epoch [15/30], Step [700/1875], Loss: 0.0883\n",
            "Epoch [15/30], Step [800/1875], Loss: 0.0126\n",
            "Epoch [15/30], Step [900/1875], Loss: 0.0146\n",
            "Epoch [15/30], Step [1000/1875], Loss: 0.0110\n",
            "Epoch [15/30], Step [1100/1875], Loss: 0.2074\n",
            "Epoch [15/30], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [15/30], Step [1300/1875], Loss: 0.0192\n",
            "Epoch [15/30], Step [1400/1875], Loss: 0.0006\n",
            "Epoch [15/30], Step [1500/1875], Loss: 0.1205\n",
            "Epoch [15/30], Step [1600/1875], Loss: 0.0020\n",
            "Epoch [15/30], Step [1700/1875], Loss: 0.0003\n",
            "Epoch [15/30], Step [1800/1875], Loss: 0.0771\n",
            "Epoch [16/30], Step [100/1875], Loss: 0.2130\n",
            "Epoch [16/30], Step [200/1875], Loss: 0.0078\n",
            "Epoch [16/30], Step [300/1875], Loss: 0.3243\n",
            "Epoch [16/30], Step [400/1875], Loss: 0.0010\n",
            "Epoch [16/30], Step [500/1875], Loss: 0.0654\n",
            "Epoch [16/30], Step [600/1875], Loss: 0.0064\n",
            "Epoch [16/30], Step [700/1875], Loss: 0.2051\n",
            "Epoch [16/30], Step [800/1875], Loss: 0.0002\n",
            "Epoch [16/30], Step [900/1875], Loss: 0.0052\n",
            "Epoch [16/30], Step [1000/1875], Loss: 0.0313\n",
            "Epoch [16/30], Step [1100/1875], Loss: 0.0044\n",
            "Epoch [16/30], Step [1200/1875], Loss: 0.0226\n",
            "Epoch [16/30], Step [1300/1875], Loss: 0.2177\n",
            "Epoch [16/30], Step [1400/1875], Loss: 0.0005\n",
            "Epoch [16/30], Step [1500/1875], Loss: 0.0006\n",
            "Epoch [16/30], Step [1600/1875], Loss: 0.0429\n",
            "Epoch [16/30], Step [1700/1875], Loss: 0.0001\n",
            "Epoch [16/30], Step [1800/1875], Loss: 0.1044\n",
            "Epoch [17/30], Step [100/1875], Loss: 0.0000\n",
            "Epoch [17/30], Step [200/1875], Loss: 0.0020\n",
            "Epoch [17/30], Step [300/1875], Loss: 0.0000\n",
            "Epoch [17/30], Step [400/1875], Loss: 0.0027\n",
            "Epoch [17/30], Step [500/1875], Loss: 0.0004\n",
            "Epoch [17/30], Step [600/1875], Loss: 0.0041\n",
            "Epoch [17/30], Step [700/1875], Loss: 0.1293\n",
            "Epoch [17/30], Step [800/1875], Loss: 0.0044\n",
            "Epoch [17/30], Step [900/1875], Loss: 0.0000\n",
            "Epoch [17/30], Step [1000/1875], Loss: 0.0002\n",
            "Epoch [17/30], Step [1100/1875], Loss: 0.1033\n",
            "Epoch [17/30], Step [1200/1875], Loss: 0.1159\n",
            "Epoch [17/30], Step [1300/1875], Loss: 0.1155\n",
            "Epoch [17/30], Step [1400/1875], Loss: 0.1985\n",
            "Epoch [17/30], Step [1500/1875], Loss: 0.0001\n",
            "Epoch [17/30], Step [1600/1875], Loss: 0.0041\n",
            "Epoch [17/30], Step [1700/1875], Loss: 0.0001\n",
            "Epoch [17/30], Step [1800/1875], Loss: 0.0733\n",
            "Epoch [18/30], Step [100/1875], Loss: 0.0000\n",
            "Epoch [18/30], Step [200/1875], Loss: 0.0169\n",
            "Epoch [18/30], Step [300/1875], Loss: 0.0018\n",
            "Epoch [18/30], Step [400/1875], Loss: 0.3344\n",
            "Epoch [18/30], Step [500/1875], Loss: 0.0004\n",
            "Epoch [18/30], Step [600/1875], Loss: 0.0014\n",
            "Epoch [18/30], Step [700/1875], Loss: 0.0010\n",
            "Epoch [18/30], Step [800/1875], Loss: 0.4145\n",
            "Epoch [18/30], Step [900/1875], Loss: 0.2590\n",
            "Epoch [18/30], Step [1000/1875], Loss: 0.2255\n",
            "Epoch [18/30], Step [1100/1875], Loss: 0.0831\n",
            "Epoch [18/30], Step [1200/1875], Loss: 0.0044\n",
            "Epoch [18/30], Step [1300/1875], Loss: 0.1932\n",
            "Epoch [18/30], Step [1400/1875], Loss: 0.0149\n",
            "Epoch [18/30], Step [1500/1875], Loss: 0.1701\n",
            "Epoch [18/30], Step [1600/1875], Loss: 0.0000\n",
            "Epoch [18/30], Step [1700/1875], Loss: 0.0017\n",
            "Epoch [18/30], Step [1800/1875], Loss: 0.0000\n",
            "Epoch [19/30], Step [100/1875], Loss: 0.0002\n",
            "Epoch [19/30], Step [200/1875], Loss: 0.0742\n",
            "Epoch [19/30], Step [300/1875], Loss: 0.2074\n",
            "Epoch [19/30], Step [400/1875], Loss: 0.0053\n",
            "Epoch [19/30], Step [500/1875], Loss: 0.2464\n",
            "Epoch [19/30], Step [600/1875], Loss: 0.1045\n",
            "Epoch [19/30], Step [700/1875], Loss: 0.0662\n",
            "Epoch [19/30], Step [800/1875], Loss: 0.0946\n",
            "Epoch [19/30], Step [900/1875], Loss: 0.0014\n",
            "Epoch [19/30], Step [1000/1875], Loss: 0.0161\n",
            "Epoch [19/30], Step [1100/1875], Loss: 0.0001\n",
            "Epoch [19/30], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [19/30], Step [1300/1875], Loss: 0.0004\n",
            "Epoch [19/30], Step [1400/1875], Loss: 0.0008\n",
            "Epoch [19/30], Step [1500/1875], Loss: 0.0110\n",
            "Epoch [19/30], Step [1600/1875], Loss: 0.0028\n",
            "Epoch [19/30], Step [1700/1875], Loss: 0.0003\n",
            "Epoch [19/30], Step [1800/1875], Loss: 0.0520\n",
            "Epoch [20/30], Step [100/1875], Loss: 0.0018\n",
            "Epoch [20/30], Step [200/1875], Loss: 0.0034\n",
            "Epoch [20/30], Step [300/1875], Loss: 0.3233\n",
            "Epoch [20/30], Step [400/1875], Loss: 0.3392\n",
            "Epoch [20/30], Step [500/1875], Loss: 0.1196\n",
            "Epoch [20/30], Step [600/1875], Loss: 0.0801\n",
            "Epoch [20/30], Step [700/1875], Loss: 0.1028\n",
            "Epoch [20/30], Step [800/1875], Loss: 0.0014\n",
            "Epoch [20/30], Step [900/1875], Loss: 0.1111\n",
            "Epoch [20/30], Step [1000/1875], Loss: 0.3014\n",
            "Epoch [20/30], Step [1100/1875], Loss: 0.0003\n",
            "Epoch [20/30], Step [1200/1875], Loss: 0.0002\n",
            "Epoch [20/30], Step [1300/1875], Loss: 0.7599\n",
            "Epoch [20/30], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [20/30], Step [1500/1875], Loss: 0.4501\n",
            "Epoch [20/30], Step [1600/1875], Loss: 0.1719\n",
            "Epoch [20/30], Step [1700/1875], Loss: 0.0113\n",
            "Epoch [20/30], Step [1800/1875], Loss: 0.0001\n",
            "Epoch [21/30], Step [100/1875], Loss: 0.0002\n",
            "Epoch [21/30], Step [200/1875], Loss: 0.0027\n",
            "Epoch [21/30], Step [300/1875], Loss: 0.0023\n",
            "Epoch [21/30], Step [400/1875], Loss: 0.0000\n",
            "Epoch [21/30], Step [500/1875], Loss: 0.0039\n",
            "Epoch [21/30], Step [600/1875], Loss: 0.0180\n",
            "Epoch [21/30], Step [700/1875], Loss: 0.0307\n",
            "Epoch [21/30], Step [800/1875], Loss: 0.0129\n",
            "Epoch [21/30], Step [900/1875], Loss: 0.0002\n",
            "Epoch [21/30], Step [1000/1875], Loss: 0.0003\n",
            "Epoch [21/30], Step [1100/1875], Loss: 0.0231\n",
            "Epoch [21/30], Step [1200/1875], Loss: 0.3069\n",
            "Epoch [21/30], Step [1300/1875], Loss: 0.0803\n",
            "Epoch [21/30], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [21/30], Step [1500/1875], Loss: 0.0004\n",
            "Epoch [21/30], Step [1600/1875], Loss: 0.0000\n",
            "Epoch [21/30], Step [1700/1875], Loss: 0.0005\n",
            "Epoch [21/30], Step [1800/1875], Loss: 0.0138\n",
            "Epoch [22/30], Step [100/1875], Loss: 0.0003\n",
            "Epoch [22/30], Step [200/1875], Loss: 0.0026\n",
            "Epoch [22/30], Step [300/1875], Loss: 0.0016\n",
            "Epoch [22/30], Step [400/1875], Loss: 0.0154\n",
            "Epoch [22/30], Step [500/1875], Loss: 0.0000\n",
            "Epoch [22/30], Step [600/1875], Loss: 0.0000\n",
            "Epoch [22/30], Step [700/1875], Loss: 0.0001\n",
            "Epoch [22/30], Step [800/1875], Loss: 0.0114\n",
            "Epoch [22/30], Step [900/1875], Loss: 0.0010\n",
            "Epoch [22/30], Step [1000/1875], Loss: 0.2756\n",
            "Epoch [22/30], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [22/30], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [22/30], Step [1300/1875], Loss: 0.0001\n",
            "Epoch [22/30], Step [1400/1875], Loss: 0.0954\n",
            "Epoch [22/30], Step [1500/1875], Loss: 0.0022\n",
            "Epoch [22/30], Step [1600/1875], Loss: 0.0011\n",
            "Epoch [22/30], Step [1700/1875], Loss: 0.4185\n",
            "Epoch [22/30], Step [1800/1875], Loss: 0.0084\n",
            "Epoch [23/30], Step [100/1875], Loss: 0.0000\n",
            "Epoch [23/30], Step [200/1875], Loss: 0.0005\n",
            "Epoch [23/30], Step [300/1875], Loss: 0.0504\n",
            "Epoch [23/30], Step [400/1875], Loss: 0.0000\n",
            "Epoch [23/30], Step [500/1875], Loss: 0.0000\n",
            "Epoch [23/30], Step [600/1875], Loss: 0.1405\n",
            "Epoch [23/30], Step [700/1875], Loss: 0.0001\n",
            "Epoch [23/30], Step [800/1875], Loss: 0.0000\n",
            "Epoch [23/30], Step [900/1875], Loss: 0.0004\n",
            "Epoch [23/30], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [23/30], Step [1100/1875], Loss: 0.5011\n",
            "Epoch [23/30], Step [1200/1875], Loss: 0.0506\n",
            "Epoch [23/30], Step [1300/1875], Loss: 0.1061\n",
            "Epoch [23/30], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [23/30], Step [1500/1875], Loss: 0.0208\n",
            "Epoch [23/30], Step [1600/1875], Loss: 0.9156\n",
            "Epoch [23/30], Step [1700/1875], Loss: 0.0041\n",
            "Epoch [23/30], Step [1800/1875], Loss: 0.0151\n",
            "Epoch [24/30], Step [100/1875], Loss: 0.0499\n",
            "Epoch [24/30], Step [200/1875], Loss: 0.0008\n",
            "Epoch [24/30], Step [300/1875], Loss: 0.0000\n",
            "Epoch [24/30], Step [400/1875], Loss: 0.3783\n",
            "Epoch [24/30], Step [500/1875], Loss: 0.0002\n",
            "Epoch [24/30], Step [600/1875], Loss: 0.0000\n",
            "Epoch [24/30], Step [700/1875], Loss: 0.0000\n",
            "Epoch [24/30], Step [800/1875], Loss: 0.0007\n",
            "Epoch [24/30], Step [900/1875], Loss: 0.0014\n",
            "Epoch [24/30], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [24/30], Step [1100/1875], Loss: 0.0001\n",
            "Epoch [24/30], Step [1200/1875], Loss: 0.0052\n",
            "Epoch [24/30], Step [1300/1875], Loss: 0.0505\n",
            "Epoch [24/30], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [24/30], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [24/30], Step [1600/1875], Loss: 0.0001\n",
            "Epoch [24/30], Step [1700/1875], Loss: 0.0063\n",
            "Epoch [24/30], Step [1800/1875], Loss: 0.0939\n",
            "Epoch [25/30], Step [100/1875], Loss: 0.0693\n",
            "Epoch [25/30], Step [200/1875], Loss: 0.0000\n",
            "Epoch [25/30], Step [300/1875], Loss: 0.0000\n",
            "Epoch [25/30], Step [400/1875], Loss: 0.0155\n",
            "Epoch [25/30], Step [500/1875], Loss: 0.0111\n",
            "Epoch [25/30], Step [600/1875], Loss: 0.0011\n",
            "Epoch [25/30], Step [700/1875], Loss: 0.0287\n",
            "Epoch [25/30], Step [800/1875], Loss: 0.0242\n",
            "Epoch [25/30], Step [900/1875], Loss: 0.0000\n",
            "Epoch [25/30], Step [1000/1875], Loss: 0.0071\n",
            "Epoch [25/30], Step [1100/1875], Loss: 0.1460\n",
            "Epoch [25/30], Step [1200/1875], Loss: 0.1798\n",
            "Epoch [25/30], Step [1300/1875], Loss: 0.0002\n",
            "Epoch [25/30], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [25/30], Step [1500/1875], Loss: 0.4816\n",
            "Epoch [25/30], Step [1600/1875], Loss: 0.2165\n",
            "Epoch [25/30], Step [1700/1875], Loss: 0.0597\n",
            "Epoch [25/30], Step [1800/1875], Loss: 0.1023\n",
            "Epoch [26/30], Step [100/1875], Loss: 0.4116\n",
            "Epoch [26/30], Step [200/1875], Loss: 0.0007\n",
            "Epoch [26/30], Step [300/1875], Loss: 0.3698\n",
            "Epoch [26/30], Step [400/1875], Loss: 0.0002\n",
            "Epoch [26/30], Step [500/1875], Loss: 0.0000\n",
            "Epoch [26/30], Step [600/1875], Loss: 0.0000\n",
            "Epoch [26/30], Step [700/1875], Loss: 0.3000\n",
            "Epoch [26/30], Step [800/1875], Loss: 0.0030\n",
            "Epoch [26/30], Step [900/1875], Loss: 0.0000\n",
            "Epoch [26/30], Step [1000/1875], Loss: 0.0002\n",
            "Epoch [26/30], Step [1100/1875], Loss: 0.0254\n",
            "Epoch [26/30], Step [1200/1875], Loss: 0.0020\n",
            "Epoch [26/30], Step [1300/1875], Loss: 0.1660\n",
            "Epoch [26/30], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [26/30], Step [1500/1875], Loss: 0.0164\n",
            "Epoch [26/30], Step [1600/1875], Loss: 0.0486\n",
            "Epoch [26/30], Step [1700/1875], Loss: 0.2385\n",
            "Epoch [26/30], Step [1800/1875], Loss: 0.0004\n",
            "Epoch [27/30], Step [100/1875], Loss: 0.0000\n",
            "Epoch [27/30], Step [200/1875], Loss: 0.0000\n",
            "Epoch [27/30], Step [300/1875], Loss: 0.0169\n",
            "Epoch [27/30], Step [400/1875], Loss: 0.0000\n",
            "Epoch [27/30], Step [500/1875], Loss: 0.0000\n",
            "Epoch [27/30], Step [600/1875], Loss: 0.5619\n",
            "Epoch [27/30], Step [700/1875], Loss: 0.0007\n",
            "Epoch [27/30], Step [800/1875], Loss: 0.0008\n",
            "Epoch [27/30], Step [900/1875], Loss: 0.0001\n",
            "Epoch [27/30], Step [1000/1875], Loss: 0.0291\n",
            "Epoch [27/30], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [27/30], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [27/30], Step [1300/1875], Loss: 0.0575\n",
            "Epoch [27/30], Step [1400/1875], Loss: 0.0003\n",
            "Epoch [27/30], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [27/30], Step [1600/1875], Loss: 0.0359\n",
            "Epoch [27/30], Step [1700/1875], Loss: 0.0052\n",
            "Epoch [27/30], Step [1800/1875], Loss: 0.0569\n",
            "Epoch [28/30], Step [100/1875], Loss: 0.0113\n",
            "Epoch [28/30], Step [200/1875], Loss: 0.0002\n",
            "Epoch [28/30], Step [300/1875], Loss: 0.2347\n",
            "Epoch [28/30], Step [400/1875], Loss: 0.0000\n",
            "Epoch [28/30], Step [500/1875], Loss: 0.0001\n",
            "Epoch [28/30], Step [600/1875], Loss: 0.4270\n",
            "Epoch [28/30], Step [700/1875], Loss: 0.0101\n",
            "Epoch [28/30], Step [800/1875], Loss: 0.0001\n",
            "Epoch [28/30], Step [900/1875], Loss: 0.0043\n",
            "Epoch [28/30], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [28/30], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [28/30], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [28/30], Step [1300/1875], Loss: 0.0959\n",
            "Epoch [28/30], Step [1400/1875], Loss: 0.0005\n",
            "Epoch [28/30], Step [1500/1875], Loss: 0.0230\n",
            "Epoch [28/30], Step [1600/1875], Loss: 0.0134\n",
            "Epoch [28/30], Step [1700/1875], Loss: 0.0001\n",
            "Epoch [28/30], Step [1800/1875], Loss: 0.0001\n",
            "Epoch [29/30], Step [100/1875], Loss: 0.0360\n",
            "Epoch [29/30], Step [200/1875], Loss: 0.0005\n",
            "Epoch [29/30], Step [300/1875], Loss: 0.2222\n",
            "Epoch [29/30], Step [400/1875], Loss: 0.0003\n",
            "Epoch [29/30], Step [500/1875], Loss: 0.0000\n",
            "Epoch [29/30], Step [600/1875], Loss: 0.0000\n",
            "Epoch [29/30], Step [700/1875], Loss: 0.0000\n",
            "Epoch [29/30], Step [800/1875], Loss: 0.0000\n",
            "Epoch [29/30], Step [900/1875], Loss: 0.0701\n",
            "Epoch [29/30], Step [1000/1875], Loss: 0.5117\n",
            "Epoch [29/30], Step [1100/1875], Loss: 0.0062\n",
            "Epoch [29/30], Step [1200/1875], Loss: 0.0287\n",
            "Epoch [29/30], Step [1300/1875], Loss: 0.0510\n",
            "Epoch [29/30], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [29/30], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [29/30], Step [1600/1875], Loss: 0.3699\n",
            "Epoch [29/30], Step [1700/1875], Loss: 0.0008\n",
            "Epoch [29/30], Step [1800/1875], Loss: 0.1756\n",
            "Epoch [30/30], Step [100/1875], Loss: 0.0065\n",
            "Epoch [30/30], Step [200/1875], Loss: 0.0618\n",
            "Epoch [30/30], Step [300/1875], Loss: 0.2537\n",
            "Epoch [30/30], Step [400/1875], Loss: 0.0131\n",
            "Epoch [30/30], Step [500/1875], Loss: 0.0000\n",
            "Epoch [30/30], Step [600/1875], Loss: 0.1447\n",
            "Epoch [30/30], Step [700/1875], Loss: 0.0096\n",
            "Epoch [30/30], Step [800/1875], Loss: 0.0017\n",
            "Epoch [30/30], Step [900/1875], Loss: 0.0001\n",
            "Epoch [30/30], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [30/30], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [30/30], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [30/30], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [30/30], Step [1400/1875], Loss: 0.0089\n",
            "Epoch [30/30], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [30/30], Step [1600/1875], Loss: 0.0002\n",
            "Epoch [30/30], Step [1700/1875], Loss: 0.0110\n",
            "Epoch [30/30], Step [1800/1875], Loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    # получаем батч\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # запишем данные в строку\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backprpagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        # считаем градиенты\n",
        "        loss.backward()\n",
        "        # обновляем параметры\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LlnDk3TkrYS2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 97.19 %\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# test\n",
        "# так как мы не использовали софтмакс, то берем максимум от выхода сети\n",
        "# и считаем что индекс этого класса и есть наш ответ\n",
        "# для интерпретируемости можно при тестировании добавить SM на выход\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "84TJdPOd4tku"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(7, device='cuda:0')\n",
            "tensor(3, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "Test accuracy is 97.19\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "batch_counter = 0\n",
        "\n",
        "total = 0\n",
        "correct = 0\n",
        "for images, labels in test_loader:\n",
        "\n",
        "    images_flat = images.reshape(-1, 28 * 28).to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images_flat)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    # отрисуем несколько примеров и выведем наши ответы на них\n",
        "    if (batch_counter < 3):\n",
        "        plt.figure(batch_counter)\n",
        "        plt.imshow(images[0][0])\n",
        "        print(predicted.data[0])\n",
        "    batch_counter += 1\n",
        "\n",
        "print(\"Test accuracy is {}\".format(100 * correct/total))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scXJ6C_XE4fm"
      },
      "source": [
        "Если точность на тесте больше 0.8 — поздравляем с выполнением задания!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Module3_final_task_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
